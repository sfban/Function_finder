** Question 1: Your pipeline can be used to index vast codebase or catalog, is it correct?

Answer: Yes. Adding index is good for fast querying. 
The dis-advantage of index is that it could make the insertion slower. 
So, it it better to add index on an finished table.

This pipline was used to extract useful information from big codebase, and to provide fast search for user.


** Question 2: How did you find that working with JSON is better?

Answer: I was first trying CSV format, using sc.textFile() to readin the data from AWS s3 bucket. 
It treated each line as individual RDD. This way, I can not identify the functions's class name and output, 
those are in different lines of the code content.

So, I using dataframe API, Spark SQL, sqlContext.read.json() to readin the data from AWS s3 in Json format.
It carried the structure of the data. While Spark SQL only support three types of data source, including 
Parquet (This is default), Json, and JDBC. CSV is not supported in Spark SQL. 

Just googled, as of Spark version 2.0 and up, spark-csv is part of core Spark functionality and doesn't require 
a separate library. One could just do for example 
df = spark.read.format("csv").option("header", "true").load("csvfile.csv"). 
I haven't tried this.


** Question 3: Are you searching only in the latest commits?

Answer: I was useing the content of the source code, the lastest updated one. 
Yes, it should be samilar as the lastest commits. 


** QUestion 4: Are you using deltas or full updates?

Answer: My pipleine is batch processing, only historical data. It could be an extension to add new updateds.


** Question 5: You mentioned extension. If you add extension, how would you do that?

Answer: I mentioned several extensions. For example, to identify resursion functions. 
By defination, recursion function is a function calls itself. The pipelien will scan the contant 
of the code, to find the same function name, then it can be a resursion function.  
Another example is the import packages, this information is easy to extract from the source code, after keyword import.
And they are valueable informations to be included in the table.
And I was also think about to deal with other languages, like Java.


** Question 6: Are you looking for duplicate functions within the same repo?

Answer: My pipeline was to find all the functions for one repo. 
The duplicated function names were the ones I double checked to make sure my output are correct. 


Question 7: How did you identify the class for one function?

Answer: Python structures by colons and indentation. I identified the class for one function by indentation.
My script scaned the source code line by line, saved the class name first. Then if I found the function and the position in line was bigger than the previous class, then, the function belonged to this class.


Question 8: Comment: You have developed a useful tool for hackers.

Answer: That's is a great comment. Thanks.



